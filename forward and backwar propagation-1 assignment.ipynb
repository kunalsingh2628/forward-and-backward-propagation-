{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d1604-aa4b-4b8e-baeb-aa1d130f36e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705ddd1-1a36-4553-9b91-0dc97bd5b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "Forward propagation is the process by which input data is passed through the neural network to generate an output. \n",
    "The purpose of forward propagation is to compute the predicted output of the neural network given a set of input features. It involves the transformation of input data through the network's layers, where each layer consists of neurons with associated weights and biases. The final output is then compared to the actual target values during the training process to adjust the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264b30f-eb62-44fc-98b5-8c33f6d4ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164e6f8-1a87-4539-86a2-864748654f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a single-layer feedforward neural network, also known as a perceptron, the mathematical implementation of forward propagation involves the following steps:\n",
    "\n",
    "Input Layer:\n",
    "\n",
    "Let \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "�\n",
    "x \n",
    "1\n",
    "​\n",
    " ,x \n",
    "2\n",
    "​\n",
    " ,...,x \n",
    "n\n",
    "​\n",
    "  be the input features.\n",
    "The input layer simply passes these features to the next layer.\n",
    "Weighted Sum:\n",
    "\n",
    "Each input is multiplied by its corresponding weight: \n",
    "�\n",
    "=\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "z=w \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +w \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +...+w \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " , where \n",
    "�\n",
    "�\n",
    "w \n",
    "i\n",
    "​\n",
    "  is the weight associated with input \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    " .\n",
    "Activation Function:\n",
    "\n",
    "The weighted sum is then passed through an activation function, denoted as \n",
    "�\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "a=f(z).\n",
    "The activation function introduces non-linearity to the model.\n",
    "Output:\n",
    "\n",
    "The output of the activation function is the final output of the neural network.\n",
    "Mathematically, the entire process can be summarized as \n",
    "output\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    ")\n",
    "output=f(w \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +w \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +...+w \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f477f5-e9ee-4081-8243-bdd06c91691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941822f-6086-4dc7-b3d9-a03bc5c280e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation functions introduce non-linearity to the neural network, enabling it to learn complex patterns and relationships in the data. Without activation functions, the entire neural network would behave like a linear model, regardless of its depth. Common activation functions include:\n",
    "\n",
    "Sigmoid Function (\n",
    "�\n",
    "σ): \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "f(z)= \n",
    "1+e \n",
    "−z\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "Hyperbolic Tangent Function (\n",
    "tanh\n",
    "⁡\n",
    "tanh): \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "tanh\n",
    "⁡\n",
    "(\n",
    "�\n",
    ")\n",
    "f(z)=tanh(z)\n",
    "Rectified Linear Unit (ReLU): \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "�\n",
    ")\n",
    "f(z)=max(0,z)\n",
    "Softmax (used in output layer for multiclass classification): \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "f(z) \n",
    "i\n",
    "​\n",
    " = \n",
    "∑ \n",
    "j=1\n",
    "N\n",
    "​\n",
    " e \n",
    "z \n",
    "j\n",
    "​\n",
    " \n",
    " \n",
    "e \n",
    "z \n",
    "i\n",
    "​\n",
    " \n",
    " \n",
    "​\n",
    " , where \n",
    "�\n",
    "N is the number of classes.\n",
    "Activation functions enable the network to capture complex relationships and learn non-linear mappings between inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5943aa-1c64-4246-af42-d98a28e55847",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01978001-b409-41a4-b7ad-5458be4deae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights (\n",
    "�\n",
    "w): The weights are parameters associated with the connections between neurons in different layers. During forward propagation, the input features are multiplied by their corresponding weights to compute a weighted sum. The weights are adjusted during the training process to minimize the difference between the predicted and actual outputs.\n",
    "\n",
    "Biases (\n",
    "�\n",
    "b): Biases are additional parameters in each neuron that allow the network to shift the output. They are added to the weighted sum before passing through the activation function. Like weights, biases are also adjusted during training to improve the model's performance.\n",
    "\n",
    "In summary, weights and biases play a crucial role in the forward propagation process by determining how input features are combined and transformed within the neural network to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5d5a3-0b7a-4763-8033-cc0e72c3d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11163d7d-19e8-4b99-aed8-42e0fba39ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7485ecd-5982-4177-8a0a-5088dd410ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5082192-1a6f-45be-bc4d-c62a7ae1e8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae6d2c-365f-4dc7-910e-0eb4abe92e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddfd5a-f9fe-459d-9dcb-d7963b5cef45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b69056-b5f5-40e0-80d9-4ee6a644180c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f42c16-11c7-42e1-9fd9-5214d0f343f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdece74a-9c40-4894-8eb0-1e6a3fe48010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779fa97c-c26e-4dfe-b8dd-0fa2e0cad26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd628ce-b973-43e4-a552-2e73343e1aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e87175-e648-4da6-a665-fed7e5a0f476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66439650-7ab5-4115-8a3f-b7cee021f2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb863568-6276-4dd1-b2a6-eacd15580074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbc20e-d041-4c57-8817-5f3c3a60b518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34d299-d4f0-42e0-ab30-25e64b47d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
